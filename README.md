

# ðŸ“¸ Memora â€“ AI-Powered Image Captioning  
**Making memories visible, making images accessible**

Memora is a **React Native (Expo) mobile application** that automatically generates meaningful, accessibility-focused image descriptions using **Google Gemini 3.0**. It is built to make visual content understandable through screen readers â€” seamlessly, privately, and at scale.

Unlike traditional captioning tools, Memora embeds descriptions directly into image metadata, ensuring accessibility persists across apps, platforms, and devices rather than remaining confined to a single application.

---

## ðŸŒ± Inspiration

The digital world is increasingly visual. Photos are central to communication, education, and memory â€” yet for many users, images remain inaccessible without meaningful descriptions. We repeatedly observed how shared images lack context, how educational diagrams cannot be interpreted by screen readers, and how personal photo libraries become collections of â€œunknown imageâ€ files.

Memora was created to address this gap by making accessibility **automatic rather than optional**, and by designing with real usage constraints in mind â€” privacy, simplicity, and reliability.

Our guiding principle is simple:

> **Accessibility should happen by default, not on request.**

---

## ðŸ¤– What Memora Does

Memora continuously monitors a userâ€™s photo library and automatically generates two complementary forms of image descriptions:

- **Concise alt text** optimized for screen readers  
- **Detailed contextual descriptions** that explain objects, people, text, and spatial relationships  

Using **Gemini 3.0â€™s vision-language capabilities**, Memora understands both visual content and embedded text, producing descriptions that go beyond generic labels.

These captions are embedded directly into the imageâ€™s **EXIF/XMP metadata**, allowing screen readers such as **TalkBack** and **VoiceOver** to read them instantly â€” across galleries, messaging apps, and photo platforms.

---

## ðŸŽ¥ Demo

â–¶ï¸ **Watch the live demo:**  
https://www.youtube.com/watch?v=1oNDaPndQXo

The demo walks through:
- Automatic image detection  
- Caption generation using Gemini 3.0  
- Metadata-based accessibility  
- Screen reader output in real time  

---

## âœ¨ Key Features

- ðŸ¤– **Automatic Captioning** â€“ Images are described as soon as they appear  
- ðŸ“± **Batch Processing** â€“ Multiple images can be processed together  
- ðŸ“ **Detailed Descriptions** â€“ Context-rich narratives for accessibility  
- ðŸ”„ **Reprocessing Support** â€“ Update captions when needed  
- âš¡ **Background Execution** â€“ Hands-free, scheduled processing  
- ðŸ›¡ï¸ **Privacy-First Design** â€“ Images are never stored externally  
- ðŸŒ™ **Dark Mode** â€“ Adaptive theming  
- â™¿ **Accessibility-First UI** â€“ Optimized for screen readers and touch navigation  
- â˜ï¸ **Optional Google Photos Sync** *(planned)*  

---

## ðŸ› ï¸ Technology & Architecture

Memora is built with a focus on reliability, modularity, and accessibility:

### Mobile & Frontend
- **React Native** (Expo SDK 51)  
- **TypeScript**  
- Redux Toolkit + Redux Persist  
- React Navigation  

### AI & Multimodal Processing
- **Google Gemini 3.0** for image understanding and caption generation  
- OCR pipelines for extracting text from images  
- Accessibility-focused prompting to avoid generic descriptions  

### Accessibility & System Design
- Background image detection using `expo-background-fetch`  
- EXIF / XMP metadata embedding for persistent accessibility  
- Native Text-to-Speech and Screen Reader APIs  
- Local-first storage with explicit user consent for any cloud interaction  

This architecture allows each component to be tested independently while ensuring the end-to-end flow remains functional and responsive.

---

## ðŸš€ How It Works (End-to-End)

1. A new image is captured or added to the device  
2. Background tasks detect the image automatically  
3. Gemini 3.0 analyzes visual and textual content  
4. Alt text and detailed descriptions are generated  
5. Captions are embedded into image metadata  
6. Screen readers can immediately read the description  

No manual uploads. No repeated actions.

---

## ðŸŒ Impact

Memora addresses a widespread and meaningful accessibility gap in everyday digital interactions. By making images understandable by default, it supports:

- Inclusive education through accessible diagrams and notes  
- Independent access to personal memories and shared media  
- Scalable accessibility across devices and platforms  

The solution is designed to work across age groups â€” from students in classrooms to older users revisiting lifelong memories â€” without requiring technical expertise.

---

## ðŸ”® Future Direction

Planned extensions include:
- On-device inference for low-connectivity environments  
- Expanded multilingual support  
- Accessibility for short videos and educational visuals  
- Deeper integration with messaging and photo platforms  
- Collaboration with accessibility organizations and schools  

---
## ðŸ“¦ Getting Started

### Prerequisites

Before you begin, ensure you have the following installed or set up:

* **Node.js 18+**
* **Expo CLI**:
    ```bash
    npm install -g @expo/cli
    ```
* **Gemini API key**
* **Android / iOS device** or simulator

### Installation

1.  **Clone the repository and install dependencies:**
    ```bash
    git clone [https://github.com/ReservedSnow673/Memora.git](https://github.com/ReservedSnow673/Memora.git)
    cd "Memora 2.0"
    npm install
    cp .env.example .env
    ```

2.  **Add your API key:**
    Open the `.env` file and paste your key:
    ```env
    GEMINI_API_KEY=your_gemini_api_key_here
    ```

3.  **Run the app:**
    ```bash
    npx expo start
    ```

---

## ðŸ‘¨â€ðŸ’» Team

* **ReservedSnow673** â€¢ [GitHub Profile](https://github.com/ReservedSnow673)
* **Pranav435** â€¢ [GitHub Profile](https://github.com/Pranav435)

---

## ðŸ™ Acknowledgments

* **Google Gemini team** for the Vision API
* **Expo team** for the React Native framework
* **The accessibility community** for continuous guidance and feedback

---

> *Built with care for accessibility, inclusion, and real-world impact.*
