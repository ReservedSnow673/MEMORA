/**
 * Memora Vision Lite v0.5
 * Comprehensive Unit Tests
 * 
 * Tests cover:
 * - Empty images
 * - Low confidence cases
 * - Text-heavy images
 * - Multiple object scenes
 * - Pipeline determinism
 */

import {
  VisionLitePipeline,
  getVisionLitePipeline,
  processImage,
  ImageBitmap,
  VisionResult,
  DEFAULT_PIPELINE_CONFIG,
  MINIMAL_SAFE_CAPTION,
  PIPELINE_VERSION,
} from '../../src/services/visionLite';

import { ImageNormalizationService } from '../../src/services/visionLite/imageNormalization';
import { ClassificationService } from '../../src/services/visionLite/classificationService';
import { ObjectDetectionService } from '../../src/services/visionLite/objectDetection';
import { OCRService } from '../../src/services/visionLite/ocrService';
import { SemanticNormalizationService } from '../../src/services/visionLite/semanticNormalization';
import { CaptionSynthesisService } from '../../src/services/visionLite/captionSynthesis';
import { ConfidenceScoringService } from '../../src/services/visionLite/confidenceScoring';
import { QualityGateService } from '../../src/services/visionLite/qualityGate';
import { PipelineTiming } from '../../src/services/visionLite/types';

// ============= Test Fixtures =============

/**
 * Create a mock ImageBitmap for testing
 */
function createMockImageBitmap(options: Partial<ImageBitmap> = {}): ImageBitmap {
  return {
    data: options.data || 'mock-base64-image-data',
    width: options.width || 640,
    height: options.height || 480,
    mimeType: options.mimeType || 'image/jpeg',
    orientationCorrected: options.orientationCorrected ?? true,
  };
}

/**
 * Create an empty/invalid ImageBitmap
 */
function createEmptyImageBitmap(): ImageBitmap {
  return {
    data: '',
    width: 0,
    height: 0,
    mimeType: 'image/jpeg',
    orientationCorrected: false,
  };
}

// ============= Pipeline Integration Tests =============

describe('Memora Vision Lite v0.5 - Pipeline Integration', () => {
  let pipeline: VisionLitePipeline;

  beforeEach(() => {
    pipeline = new VisionLitePipeline({ debugMode: false });
  });

  describe('Basic Pipeline Execution', () => {
    it('should process a valid image and return a result', async () => {
      const imageBitmap = createMockImageBitmap();
      const result = await pipeline.processImage(imageBitmap);

      expect(result).toBeDefined();
      expect(result.caption_text).toBeDefined();
      expect(result.confidence_score).toBeGreaterThanOrEqual(0);
      expect(result.confidence_score).toBeLessThanOrEqual(1);
      expect(result.version).toBe(PIPELINE_VERSION);
      expect(result.success).toBe(true);
    });

    it('should include stage timing information', async () => {
      const imageBitmap = createMockImageBitmap();
      const result = await pipeline.processImage(imageBitmap);

      expect(result.stageTiming).toBeDefined();
      expect(result.stageTiming.length).toBeGreaterThan(0);
      
      // Check that all stages are represented
      const stageNames = result.stageTiming.map((s: PipelineTiming) => s.stage);
      expect(stageNames).toContain('normalization');
      expect(stageNames).toContain('classification');
      expect(stageNames).toContain('detection');
    });

    it('should include signal breakdown', async () => {
      const imageBitmap = createMockImageBitmap();
      const result = await pipeline.processImage(imageBitmap);

      expect(result.signal_breakdown).toBeDefined();
      expect(result.signal_breakdown.classification).toBeDefined();
      expect(result.signal_breakdown.detection).toBeDefined();
      expect(result.signal_breakdown.ocr).toBeDefined();
      expect(result.signal_breakdown.semantic).toBeDefined();
      expect(result.signal_breakdown.qualityGate).toBeDefined();
    });
  });

  describe('Empty Image Handling', () => {
    it('should handle empty image data gracefully', async () => {
      const emptyImage = createEmptyImageBitmap();
      
      // Should throw or return error result
      await expect(async () => {
        const result = await pipeline.processImage(emptyImage);
        if (!result.success) throw new Error(result.error);
      }).rejects.toThrow();
    });

    it('should handle missing image URI', async () => {
      const result = await pipeline.processImageFromUri('/nonexistent/path/image.jpg');
      
      expect(result.success).toBe(false);
      expect(result.caption_text).toBe(MINIMAL_SAFE_CAPTION);
    });
  });

  describe('Low Confidence Cases', () => {
    it('should return result structure with quality gate assessment', async () => {
      // Test that high-threshold pipeline still provides proper result structure
      const strictPipeline = new VisionLitePipeline({
        qualityGateThreshold: 0.99, // Extremely high threshold
        debugMode: false,
      });

      const imageBitmap = createMockImageBitmap();
      const result = await strictPipeline.processImage(imageBitmap);

      // With real models returning high confidence, the quality gate may pass
      // The important thing is the structure is correct
      expect(result.signal_breakdown.qualityGate).toBeDefined();
      expect(typeof result.signal_breakdown.qualityGate.passed).toBe('boolean');
      expect(typeof result.signal_breakdown.qualityGate.threshold).toBe('number');
      expect(result.signal_breakdown.qualityGate.threshold).toBe(0.99);
      expect(typeof result.caption_text).toBe('string');
    });
  });

  describe('Determinism', () => {
    it('should produce consistent result structure for identical inputs', async () => {
      const imageBitmap = createMockImageBitmap();
      
      // Process same image multiple times
      const result1 = await pipeline.processImage(imageBitmap);
      const result2 = await pipeline.processImage(imageBitmap);

      // Note: In simulation mode with Math.random(), exact values may vary
      // In production with real ML models, outputs would be deterministic
      // Here we verify the structure is consistent
      expect(result1.version).toBe(result2.version);
      expect(result1.success).toBe(result2.success);
      expect(typeof result1.signal_breakdown.semantic.type).toBe('string');
      expect(typeof result2.signal_breakdown.semantic.type).toBe('string');
      expect(result1.stageTiming.length).toBe(result2.stageTiming.length);
    });
  });

  describe('Performance', () => {
    it('should complete within target latency', async () => {
      const imageBitmap = createMockImageBitmap();
      
      const startTime = Date.now();
      await pipeline.processImage(imageBitmap);
      const duration = Date.now() - startTime;

      // Target: 200-400ms for typical photo
      // Allow more time in test environment
      expect(duration).toBeLessThan(2000);
    });
  });
});

// ============= Image Normalization Tests =============

describe('Memora Vision Lite v0.5 - Image Normalization', () => {
  let service: ImageNormalizationService;

  beforeEach(() => {
    service = new ImageNormalizationService();
  });

  it('should normalize image to target size', async () => {
    const imageBitmap = createMockImageBitmap({ width: 1920, height: 1080 });
    const normalized = await service.normalize(imageBitmap);

    expect(normalized.width).toBe(DEFAULT_PIPELINE_CONFIG.targetImageSize);
    expect(normalized.height).toBe(DEFAULT_PIPELINE_CONFIG.targetImageSize);
    expect(normalized.tensorData).toBeDefined();
  });

  it('should preserve original dimensions in metadata', async () => {
    const imageBitmap = createMockImageBitmap({ width: 800, height: 600 });
    const normalized = await service.normalize(imageBitmap);

    expect(normalized.originalWidth).toBe(800);
    expect(normalized.originalHeight).toBe(600);
  });

  it('should handle various image sizes', async () => {
    // Test with different image sizes
    const smallImage = createMockImageBitmap({ width: 320, height: 240 });
    const normalizedSmall = await service.normalize(smallImage);
    expect(normalizedSmall.originalWidth).toBe(320);
    expect(normalizedSmall.originalHeight).toBe(240);

    const largeImage = createMockImageBitmap({ width: 4000, height: 3000 });
    const normalizedLarge = await service.normalize(largeImage);
    expect(normalizedLarge.originalWidth).toBe(4000);
    expect(normalizedLarge.originalHeight).toBe(3000);
  });

  it('should reject invalid MIME types', async () => {
    const invalidImage = {
      ...createMockImageBitmap(),
      mimeType: 'image/gif' as any,
    };

    await expect(service.normalize(invalidImage)).rejects.toThrow('Unsupported MIME type');
  });
});

// ============= Classification Service Tests =============

describe('Memora Vision Lite v0.5 - Classification Service', () => {
  let service: ClassificationService;

  beforeEach(() => {
    service = new ClassificationService();
  });

  it('should return classification labels', async () => {
    await service.initialize();
    
    const normalizationService = new ImageNormalizationService();
    const imageBitmap = createMockImageBitmap();
    const normalized = await normalizationService.normalize(imageBitmap);
    
    const result = await service.classify(normalized);

    expect(result.success).toBe(true);
    expect(result.labels).toBeDefined();
    expect(result.labels.length).toBeGreaterThan(0);
  });

  it('should filter labels below threshold', async () => {
    const strictService = new ClassificationService({
      classificationThreshold: 0.9,
    });
    await strictService.initialize();
    
    const normalizationService = new ImageNormalizationService();
    const imageBitmap = createMockImageBitmap();
    const normalized = await normalizationService.normalize(imageBitmap);
    
    const result = await strictService.classify(normalized);

    for (const label of result.labels) {
      expect(label.confidence).toBeGreaterThanOrEqual(0.9);
    }
  });

  it('should detect text indicators', () => {
    // Use ImageNet labels that MobileNet actually uses for text indicators
    const labelsWithText = [
      { label: 'notebook', normalizedLabel: 'notebook', confidence: 0.8, index: 0 },
      { label: 'desktop computer', normalizedLabel: 'desktop computer', confidence: 0.6, index: 1 },
    ];

    expect(service.hasTextIndicator(labelsWithText)).toBe(true);
  });

  it('should not detect text indicators in photo labels', () => {
    const photoLabels = [
      { label: 'person', normalizedLabel: 'person', confidence: 0.8, index: 0 },
      { label: 'outdoor', normalizedLabel: 'outdoor', confidence: 0.6, index: 1 },
    ];

    expect(service.hasTextIndicator(photoLabels)).toBe(false);
  });
});

// ============= Object Detection Tests =============

describe('Memora Vision Lite v0.5 - Object Detection', () => {
  let service: ObjectDetectionService;

  beforeEach(() => {
    service = new ObjectDetectionService();
  });

  it('should detect objects in image', async () => {
    await service.initialize();
    
    const normalizationService = new ImageNormalizationService();
    const imageBitmap = createMockImageBitmap();
    const normalized = await normalizationService.normalize(imageBitmap);
    
    const result = await service.detect(normalized);

    expect(result.success).toBe(true);
    expect(result.objects).toBeDefined();
  });

  it('should count people correctly', () => {
    const objects = [
      { label: 'person', normalizedLabel: 'person', confidence: 0.9, boundingBox: { x: 0.1, y: 0.1, width: 0.3, height: 0.7 } },
      { label: 'person', normalizedLabel: 'person', confidence: 0.8, boundingBox: { x: 0.5, y: 0.1, width: 0.3, height: 0.7 } },
      { label: 'chair', normalizedLabel: 'chair', confidence: 0.7, boundingBox: { x: 0.2, y: 0.5, width: 0.2, height: 0.3 } },
    ];

    expect(service.countPeople(objects)).toBe(2);
  });

  it('should not fail pipeline on detection error', async () => {
    // Detection is optional - should return empty result on error
    const normalizationService = new ImageNormalizationService();
    const imageBitmap = createMockImageBitmap();
    const normalized = await normalizationService.normalize(imageBitmap);
    
    const result = await service.detect(normalized);

    // Should not throw, should return result (success or graceful failure)
    expect(result).toBeDefined();
    expect(result.objects).toBeDefined();
  });
});

// ============= OCR Service Tests =============

describe('Memora Vision Lite v0.5 - OCR Service', () => {
  let service: OCRService;

  beforeEach(() => {
    service = new OCRService();
  });

  it('should trigger OCR when classification suggests text', async () => {
    const context = {
      classificationLabels: [
        { label: 'document', normalizedLabel: 'document', confidence: 0.8, index: 0 },
      ],
      userAlwaysOCR: false,
    };

    const result = service.shouldTriggerOCR(context);
    expect(result.trigger).toBe(true);
    expect(result.reason).toBe('classification_hint');
  });

  it('should trigger OCR when user enables always-OCR', async () => {
    const context = {
      classificationLabels: [
        { label: 'person', normalizedLabel: 'person', confidence: 0.8, index: 0 },
      ],
      userAlwaysOCR: true,
    };

    const result = service.shouldTriggerOCR(context);
    expect(result.trigger).toBe(true);
    expect(result.reason).toBe('user_enabled');
  });

  it('should not trigger OCR for regular photos', async () => {
    const context = {
      classificationLabels: [
        { label: 'person', normalizedLabel: 'person', confidence: 0.8, index: 0 },
        { label: 'outdoor', normalizedLabel: 'outdoor', confidence: 0.6, index: 1 },
      ],
      userAlwaysOCR: false,
    };

    const result = service.shouldTriggerOCR(context);
    expect(result.trigger).toBe(false);
    expect(result.reason).toBe('not_triggered');
  });

  it('should extract and summarize text', async () => {
    await service.initialize();
    
    const normalizationService = new ImageNormalizationService();
    const imageBitmap = createMockImageBitmap();
    const normalized = await normalizationService.normalize(imageBitmap);
    
    const context = {
      classificationLabels: [
        { label: 'document', normalizedLabel: 'document', confidence: 0.8, index: 0 },
      ],
      userAlwaysOCR: false,
    };

    const result = await service.extractText(normalized, context);

    expect(result.triggered).toBe(true);
    expect(result.textSummary.length).toBeLessThanOrEqual(DEFAULT_PIPELINE_CONFIG.maxTextSummaryLength + 3); // +3 for "..."
  });
});

// ============= Semantic Normalization Tests =============

describe('Memora Vision Lite v0.5 - Semantic Normalization', () => {
  let service: SemanticNormalizationService;

  beforeEach(() => {
    service = new SemanticNormalizationService();
  });

  it('should collapse synonyms correctly', () => {
    const classification = {
      labels: [
        { label: 'man', normalizedLabel: 'man', confidence: 0.8, index: 0 },
        { label: 'woman', normalizedLabel: 'woman', confidence: 0.7, index: 1 },
      ],
      inferenceTimeMs: 100,
      modelId: 'mobilenet_v2' as const,
      success: true,
    };

    const detection = {
      objects: [],
      inferenceTimeMs: 100,
      modelId: 'ssd_mobilenet_v2' as const,
      success: true,
    };

    const ocr = {
      triggered: false,
      triggerReason: 'not_triggered' as const,
      textBlocks: [],
      extractedText: '',
      textSummary: '',
      hasMeaningfulText: false,
      processingTimeMs: 0,
      success: true,
    };

    const semantic = service.normalize(classification, detection, ocr);

    // Both 'man' and 'woman' should be collapsed to 'person'
    expect(semantic.allLabels).toContain('person');
    expect(semantic.allLabels).not.toContain('man');
    expect(semantic.allLabels).not.toContain('woman');
  });

  it('should correctly classify image types', () => {
    const documentClassification = {
      labels: [
        { label: 'document', normalizedLabel: 'document', confidence: 0.8, index: 0 },
      ],
      inferenceTimeMs: 100,
      modelId: 'mobilenet_v2' as const,
      success: true,
    };

    const ocrWithText = {
      triggered: true,
      triggerReason: 'classification_hint' as const,
      textBlocks: [
        { text: 'Chapter 1', confidence: 0.9, boundingBox: { x: 0.1, y: 0.1, width: 0.5, height: 0.1 } },
        { text: 'Introduction', confidence: 0.85, boundingBox: { x: 0.1, y: 0.2, width: 0.4, height: 0.1 } },
        { text: 'Body text...', confidence: 0.8, boundingBox: { x: 0.1, y: 0.3, width: 0.7, height: 0.3 } },
      ],
      extractedText: 'Chapter 1 Introduction Body text...',
      textSummary: 'Chapter 1 Introduction',
      hasMeaningfulText: true,
      processingTimeMs: 200,
      success: true,
    };

    const semantic = service.normalize(
      documentClassification,
      { objects: [], inferenceTimeMs: 0, modelId: 'ssd_mobilenet_v2', success: true },
      ocrWithText
    );

    expect(semantic.type).toBe('document');
    expect(semantic.textPresent).toBe(true);
  });

  it('should generate correct person descriptor', () => {
    expect(service.getPersonDescriptor(0)).toBe('');
    expect(service.getPersonDescriptor(1)).toBe('a person');
    expect(service.getPersonDescriptor(2)).toBe('two people');
    expect(service.getPersonDescriptor(5)).toBe('5 people');
    expect(service.getPersonDescriptor(10)).toBe('a group of people');
  });
});

// ============= Caption Synthesis Tests =============

describe('Memora Vision Lite v0.5 - Caption Synthesis', () => {
  let service: CaptionSynthesisService;

  beforeEach(() => {
    service = new CaptionSynthesisService();
  });

  it('should select correct template for person photos', () => {
    const semantic = {
      type: 'photo' as const,
      primarySubjects: [],
      secondaryObjects: ['laptop'],
      environment: 'indoor' as const,
      textPresent: false,
      personCount: 1,
      allLabels: ['person', 'laptop', 'indoor'],
    };

    const selection = service.selectTemplate(semantic);
    expect(selection.template).toBe('photo_with_person');
  });

  it('should select correct template for multiple people', () => {
    const semantic = {
      type: 'photo' as const,
      primarySubjects: [],
      secondaryObjects: [],
      environment: 'outdoor' as const,
      textPresent: false,
      personCount: 3,
      allLabels: ['person', 'outdoor'],
    };

    const selection = service.selectTemplate(semantic);
    expect(selection.template).toBe('photo_with_people');
  });

  it('should select text template for documents', () => {
    const semantic = {
      type: 'document' as const,
      primarySubjects: [],
      secondaryObjects: [],
      environment: 'unknown' as const,
      textPresent: true,
      textContent: 'Chapter 1: Introduction',
      personCount: 0,
      allLabels: ['document'],
    };

    const selection = service.selectTemplate(semantic);
    expect(selection.template).toBe('document_with_text');
  });

  it('should synthesize caption within word limit', () => {
    const semantic = {
      type: 'photo' as const,
      primarySubjects: ['laptop', 'phone', 'book'],
      secondaryObjects: ['cup', 'table', 'chair', 'lamp', 'window'],
      environment: 'indoor' as const,
      textPresent: false,
      personCount: 1,
      actionContext: 'using a laptop',
      allLabels: ['person', 'laptop', 'phone', 'book', 'cup', 'table', 'chair', 'lamp', 'window', 'indoor'],
    };

    const selection = service.selectTemplate(semantic);
    const caption = service.synthesize(semantic, selection);

    expect(caption.wordCount).toBeLessThanOrEqual(DEFAULT_PIPELINE_CONFIG.maxCaptionWords);
  });

  it('should not include guesses about identity or emotions', () => {
    const semantic = {
      type: 'photo' as const,
      primarySubjects: [],
      secondaryObjects: [],
      environment: 'indoor' as const,
      textPresent: false,
      personCount: 1,
      allLabels: ['person', 'indoor'],
    };

    const selection = service.selectTemplate(semantic);
    const caption = service.synthesize(semantic, selection);

    // Should not contain identity-related words
    const forbiddenWords = ['happy', 'sad', 'angry', 'john', 'jane', 'male', 'female'];
    const captionLower = caption.text.toLowerCase();
    
    for (const word of forbiddenWords) {
      expect(captionLower).not.toContain(word);
    }
  });
});

// ============= Confidence Scoring Tests =============

describe('Memora Vision Lite v0.5 - Confidence Scoring', () => {
  let service: ConfidenceScoringService;

  beforeEach(() => {
    service = new ConfidenceScoringService();
  });

  it('should calculate confidence between 0 and 1', () => {
    const classification = {
      labels: [{ label: 'person', normalizedLabel: 'person', confidence: 0.8, index: 0 }],
      inferenceTimeMs: 100,
      modelId: 'mobilenet_v2' as const,
      success: true,
    };

    const detection = {
      objects: [{ label: 'person', normalizedLabel: 'person', confidence: 0.7, boundingBox: { x: 0.1, y: 0.1, width: 0.4, height: 0.8 } }],
      inferenceTimeMs: 100,
      modelId: 'ssd_mobilenet_v2' as const,
      success: true,
    };

    const ocr = {
      triggered: false,
      triggerReason: 'not_triggered' as const,
      textBlocks: [],
      extractedText: '',
      textSummary: '',
      hasMeaningfulText: false,
      processingTimeMs: 0,
      success: true,
    };

    const semantic = {
      type: 'photo' as const,
      primarySubjects: ['person'],
      secondaryObjects: [],
      environment: 'unknown' as const,
      textPresent: false,
      personCount: 1,
      allLabels: ['person'],
    };

    const { score } = service.calculateConfidence(classification, detection, ocr, semantic);

    expect(score).toBeGreaterThanOrEqual(0);
    expect(score).toBeLessThanOrEqual(1);
  });

  it('should return lower confidence for failed stages', () => {
    const failedClassification = {
      labels: [],
      inferenceTimeMs: 100,
      modelId: 'mobilenet_v2' as const,
      success: false,
      error: 'Classification failed',
    };

    const failedDetection = {
      objects: [],
      inferenceTimeMs: 100,
      modelId: 'ssd_mobilenet_v2' as const,
      success: false,
    };

    const ocr = {
      triggered: false,
      triggerReason: 'not_triggered' as const,
      textBlocks: [],
      extractedText: '',
      textSummary: '',
      hasMeaningfulText: false,
      processingTimeMs: 0,
      success: true,
    };

    const semantic = {
      type: 'unknown' as const,
      primarySubjects: [],
      secondaryObjects: [],
      environment: 'unknown' as const,
      textPresent: false,
      personCount: 0,
      allLabels: [],
    };

    const { score } = service.calculateConfidence(failedClassification, failedDetection, ocr, semantic);

    // When both classification and detection fail, only consistency weight remains
    // Consistency can still provide some score, so we check for low confidence (â‰¤ 0.5)
    expect(score).toBeLessThanOrEqual(0.5);
  });

  it('should categorize confidence levels correctly', () => {
    expect(service.getConfidenceLevel(0.9)).toBe('high');
    expect(service.getConfidenceLevel(0.7)).toBe('medium');
    expect(service.getConfidenceLevel(0.5)).toBe('low');
    expect(service.getConfidenceLevel(0.2)).toBe('very_low');
  });
});

// ============= Quality Gate Tests =============

describe('Memora Vision Lite v0.5 - Quality Gate', () => {
  let service: QualityGateService;

  beforeEach(() => {
    service = new QualityGateService();
  });

  it('should pass captions above threshold', () => {
    const caption = {
      text: 'A person using a laptop indoors.',
      wordCount: 6,
      template: 'photo_with_person' as const,
      substitutions: {},
    };

    const confidenceBreakdown = {
      classificationConfidence: 0.8,
      detectionConfidence: 0.7,
      ocrConfidence: 0.5,
      signalConsistency: 0.8,
      weights: { classification: 0.35, detection: 0.25, ocr: 0.15, consistency: 0.25 },
    };

    const result = service.evaluate(caption, 0.75, confidenceBreakdown);

    expect(result.passed).toBe(true);
    expect(result.recommendCloudEscalation).toBe(false);
  });

  it('should fail captions below threshold', () => {
    const caption = {
      text: 'An image.',
      wordCount: 2,
      template: 'unknown' as const,
      substitutions: {},
    };

    const confidenceBreakdown = {
      classificationConfidence: 0.2,
      detectionConfidence: 0.1,
      ocrConfidence: 0.5,
      signalConsistency: 0.3,
      weights: { classification: 0.35, detection: 0.25, ocr: 0.15, consistency: 0.25 },
    };

    const result = service.evaluate(caption, 0.25, confidenceBreakdown);

    expect(result.passed).toBe(false);
    expect(result.recommendCloudEscalation).toBe(true);
  });

  it('should return minimal caption when gate fails', () => {
    const caption = {
      text: 'Original caption.',
      wordCount: 2,
      template: 'minimal' as const,
      substitutions: {},
    };

    const gateResult = {
      passed: false,
      threshold: 0.5,
      actualConfidence: 0.3,
      recommendCloudEscalation: true,
      reason: 'Low confidence',
    };

    const { finalCaption, wasModified } = service.applyGate(caption, gateResult);

    expect(finalCaption).toBe(MINIMAL_SAFE_CAPTION);
    expect(wasModified).toBe(true);
  });
});

// ============= End-to-End Scenario Tests =============

describe('Memora Vision Lite v0.5 - E2E Scenarios', () => {
  let pipeline: VisionLitePipeline;

  beforeEach(() => {
    pipeline = new VisionLitePipeline({ debugMode: false });
  });

  describe('Multiple Object Scene', () => {
    it('should handle scenes with multiple objects', async () => {
      const imageBitmap = createMockImageBitmap();
      const result = await pipeline.processImage(imageBitmap);

      expect(result.success).toBe(true);
      expect(result.signal_breakdown.semantic.allLabels.length).toBeGreaterThan(0);
    });
  });

  describe('Text-Heavy Image', () => {
    it('should trigger OCR for text-heavy images when configured', async () => {
      const textPipeline = new VisionLitePipeline({
        alwaysRunOCR: true,
        debugMode: false,
      });

      const imageBitmap = createMockImageBitmap();
      const result = await textPipeline.processImage(imageBitmap);

      expect(result.success).toBe(true);
      expect(result.signal_breakdown.ocr.triggered).toBe(true);
    });
  });

  describe('Version and Metadata', () => {
    it('should include correct version information', async () => {
      const versionInfo = pipeline.getVersionInfo();

      expect(versionInfo.name).toBe('Memora Vision Lite');
      expect(versionInfo.version).toBe('v0.5');
    });

    it('should include timestamp in results', async () => {
      const imageBitmap = createMockImageBitmap();
      const beforeTime = Date.now();
      const result = await pipeline.processImage(imageBitmap);
      const afterTime = Date.now();

      expect(result.timestamp).toBeGreaterThanOrEqual(beforeTime);
      expect(result.timestamp).toBeLessThanOrEqual(afterTime);
    });
  });

  describe('Configuration', () => {
    it('should allow runtime configuration updates', async () => {
      const config = pipeline.getConfig();
      expect(config.qualityGateThreshold).toBe(DEFAULT_PIPELINE_CONFIG.qualityGateThreshold);

      pipeline.updateConfig({ qualityGateThreshold: 0.7 });
      const newConfig = pipeline.getConfig();
      expect(newConfig.qualityGateThreshold).toBe(0.7);
    });
  });
});
